"""
Reporting tools for generating data quality reports
"""
from typing import Dict, Any, List, Optional, Union
import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
import json
import yaml
import markdown
from jinja2 import Template
import pdfkit
from loguru import logger
import base64
from io import BytesIO, StringIO

class ReportingTools:
    """Tools for generating data quality reports"""
    
    def __init__(self, templates_dir: str = "report_templates"):
        """Initialize reporting tools"""
        self.templates_dir = Path(templates_dir)
        self.templates_dir.mkdir(exist_ok=True)
        
        # Load default templates
        self._load_default_templates()
    
    def _load_default_templates(self):
        """Load default report templates"""
        default_templates = {
            'executive_summary': """
# Data Quality Executive Summary

**Report Date:** {{ report_date }}
**Table Analyzed:** {{ table_name }}
**Total Rows:** {{ row_count|intcomma }}

## ðŸ“Š Overall Quality Score
<div style="text-align: center; margin: 20px 0;">
    <div style="font-size: 4rem; font-weight: bold; color: {{ quality_color }};">
        {{ overall_score }}%
    </div>
    <div style="font-size: 1.2rem; color: #666;">
        {{ quality_status }}
    </div>
</div>

## ðŸš¨ Critical Findings
{% if critical_issues %}
<ul>
{% for issue in critical_issues %}
<li><strong>{{ issue.title }}</strong> - {{ issue.description }}</li>
{% endfor %}
</ul>
{% else %}
<p>No critical issues found âœ…</p>
{% endif %}

## ðŸŽ¯ Recommendations
<ol>
{% for rec in recommendations %}
<li>{{ rec }}</li>
{% endfor %}
</ol>

## ðŸ“ˆ Key Metrics
| Metric | Value | Status |
|--------|-------|--------|
{% for metric in metrics %}
| {{ metric.name }} | {{ metric.value }} | {{ metric.status }} |
{% endfor %}

---

*Generated by Data Quality Agent System*
""",
            
            'detailed_report': """
# Detailed Data Quality Report

**Table:** {{ table_name }}
**Report Date:** {{ report_date }}
**Rows Analyzed:** {{ rows_analyzed|intcomma }}

## 1. Executive Summary
- **Overall Quality Score:** {{ overall_score }}%
- **Total Issues:** {{ total_issues }}
- **Critical Issues:** {{ critical_issues_count }}
- **Analysis Duration:** {{ analysis_duration }}

## 2. Data Profile
{% if profile_summary %}
### 2.1 Basic Statistics
- **Total Columns:** {{ profile_summary.column_count }}
- **Memory Usage:** {{ profile_summary.memory_mb }} MB
- **Sample Size:** {{ profile_summary.sample_size }}

### 2.2 Column Types
{% for type, count in profile_summary.column_types.items() %}
- {{ type }}: {{ count }} columns
{% endfor %}
{% endif %}

## 3. Data Quality Issues
{% if issues_by_severity %}
### 3.1 Issues by Severity
{% for severity, issues in issues_by_severity.items() %}
#### {{ severity|title }} Issues ({{ issues|length }})
{% for issue in issues %}
- **{{ issue.column }}**: {{ issue.description }}
  - Count: {{ issue.count }}
  - Percentage: {{ issue.percentage }}%
  {% if issue.sample_data %}
  - Sample: {{ issue.sample_data|join(', ') }}
  {% endif %}
{% endfor %}
{% endfor %}
{% else %}
No data quality issues detected.
{% endif %}

## 4. Statistical Analysis
{% if statistical_analysis %}
### 4.1 Numeric Columns
{% for column, stats in statistical_analysis.numeric_stats.items() %}
#### {{ column }}
- Range: {{ stats.min }} to {{ stats.max }}
- Mean: {{ stats.mean }}
- Std Dev: {{ stats.std }}
- Nulls: {{ stats.nulls }} ({{ stats.null_percentage }}%)
{% endfor %}

### 4.2 Categorical Columns
{% for column, stats in statistical_analysis.categorical_stats.items() %}
#### {{ column }}
- Unique Values: {{ stats.unique_count }}
- Most Common: {{ stats.most_common.value }} ({{ stats.most_common.count }})
- Nulls: {{ stats.nulls }} ({{ stats.null_percentage }}%)
{% endfor %}
{% endif %}

## 5. Anomaly Detection
{% if anomaly_results %}
### 5.1 Anomaly Summary
- **Total Anomalies:** {{ anomaly_results.total_anomalies }}
- **Anomaly Rate:** {{ anomaly_results.anomaly_rate }}%

### 5.2 Anomaly Types
{% for type, count in anomaly_results.anomaly_types.items() %}
- {{ type|title }}: {{ count }} anomalies
{% endfor %}
{% else %}
No anomalies detected.
{% endif %}

## 6. Recommendations & Action Plan

### 6.1 Immediate Actions (Within 24 hours)
{% for action in recommendations.immediate %}
- {{ action }}
{% endfor %}

### 6.2 Short-term Improvements (Within 1 week)
{% for action in recommendations.short_term %}
- {{ action }}
{% endfor %}

### 6.3 Long-term Strategy (Within 1 month)
{% for action in recommendations.long_term %}
- {{ action }}
{% endfor %}

## 7. Technical Details
- **Analysis Method:** {{ technical_details.analysis_method }}
- **Tools Used:** {{ technical_details.tools_used|join(', ') }}
- **Confidence Level:** {{ technical_details.confidence_level }}
- **Limitations:** {{ technical_details.limitations }}

---

*Report generated on {{ generated_timestamp }}*
*Data Quality Agent System v1.0*
"""
        }
        
        for template_name, template_content in default_templates.items():
            template_path = self.templates_dir / f"{template_name}.j2"
            if not template_path.exists():
                with open(template_path, 'w', encoding='utf-8') as f:
                    f.write(template_content)
    
    def generate_executive_summary(self, analysis_results: Dict[str, Any]) -> str:
        """Generate executive summary report"""
        try:
            # Prepare template data
            template_data = {
                'report_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'table_name': analysis_results.get('table_name', 'Unknown'),
                'row_count': analysis_results.get('row_count', 0),
                'overall_score': analysis_results.get('overall_score', 0),
                'quality_color': self._get_quality_color(analysis_results.get('overall_score', 0)),
                'quality_status': self._get_quality_status(analysis_results.get('overall_score', 0)),
                'critical_issues': analysis_results.get('critical_issues', []),
                'recommendations': analysis_results.get('recommendations', []),
                'metrics': self._extract_key_metrics(analysis_results)
            }
            
            # Load template
            template_path = self.templates_dir / "executive_summary.j2"
            with open(template_path, 'r', encoding='utf-8') as f:
                template = Template(f.read())
            
            # Render template
            html_report = template.render(**template_data)
            
            return html_report
            
        except Exception as e:
            logger.error(f"Error generating executive summary: {str(e)}")
            return f"<h1>Error Generating Report</h1><p>{str(e)}</p>"
    
    def generate_detailed_report(self, analysis_results: Dict[str, Any]) -> str:
        """Generate detailed data quality report"""
        try:
            # Prepare template data
            template_data = {
                'report_date': datetime.now().strftime('%Y-%m-%d'),
                'generated_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'table_name': analysis_results.get('table_name', 'Unknown'),
                'rows_analyzed': analysis_results.get('rows_analyzed', 0),
                'overall_score': analysis_results.get('overall_score', 0),
                'total_issues': len(analysis_results.get('all_issues', [])),
                'critical_issues_count': len([i for i in analysis_results.get('all_issues', []) 
                                            if i.get('severity') == 'critical']),
                'analysis_duration': analysis_results.get('analysis_duration', 'N/A'),
                
                'profile_summary': analysis_results.get('profile_summary', {}),
                'issues_by_severity': self._group_issues_by_severity(analysis_results.get('all_issues', [])),
                'statistical_analysis': analysis_results.get('statistical_analysis', {}),
                'anomaly_results': analysis_results.get('anomaly_results', {}),
                'recommendations': analysis_results.get('recommendations', {
                    'immediate': [],
                    'short_term': [],
                    'long_term': []
                }),
                'technical_details': analysis_results.get('technical_details', {
                    'analysis_method': 'Comprehensive Analysis',
                    'tools_used': ['Data Profiler', 'Anomaly Detector', 'Quality Analyzer'],
                    'confidence_level': 'High',
                    'limitations': 'Analysis based on sample data'
                })
            }
            
            # Load template
            template_path = self.templates_dir / "detailed_report.j2"
            with open(template_path, 'r', encoding='utf-8') as f:
                template = Template(f.read())
            
            # Add custom filters
            template.environment.filters['intcomma'] = lambda x: f"{int(x):,}"
            
            # Render template
            html_report = template.render(**template_data)
            
            return html_report
            
        except Exception as e:
            logger.error(f"Error generating detailed report: {str(e)}")
            return f"<h1>Error Generating Report</h1><p>{str(e)}</p>"
    
    def generate_markdown_report(self, analysis_results: Dict[str, Any]) -> str:
        """Generate report in Markdown format"""
        try:
            markdown_content = f"""# Data Quality Report

**Table:** {analysis_results.get('table_name', 'Unknown')}
**Report Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Overall Quality Score:** {analysis_results.get('overall_score', 0)}%

## Summary

{analysis_results.get('summary', 'No summary available')}

## Key Metrics

| Metric | Value | Status |
|--------|-------|--------|
"""
            
            # Add metrics
            for metric in self._extract_key_metrics(analysis_results):
                markdown_content += f"| {metric['name']} | {metric['value']} | {metric['status']} |\n"
            
            # Add issues
            markdown_content += "\n## Issues Found\n\n"
            issues_by_severity = self._group_issues_by_severity(analysis_results.get('all_issues', []))
            
            for severity, issues in issues_by_severity.items():
                if issues:
                    markdown_content += f"### {severity.title()} Issues\n\n"
                    for issue in issues[:5]:  # Limit to 5 issues per severity
                        markdown_content += f"- **{issue.get('column', 'Unknown')}**: {issue.get('description', 'No description')}\n"
                    if len(issues) > 5:
                        markdown_content += f"- ... and {len(issues) - 5} more\n"
                    markdown_content += "\n"
            
            # Add recommendations
            markdown_content += "## Recommendations\n\n"
            recommendations = analysis_results.get('recommendations', {})
            
            for category, items in recommendations.items():
                if items:
                    markdown_content += f"### {category.replace('_', ' ').title()}\n\n"
                    for item in items:
                        markdown_content += f"- {item}\n"
                    markdown_content += "\n"
            
            return markdown_content
            
        except Exception as e:
            logger.error(f"Error generating markdown report: {str(e)}")
            return f"# Error\n\nError generating report: {str(e)}"
    
    def generate_json_report(self, analysis_results: Dict[str, Any]) -> str:
        """Generate report in JSON format"""
        try:
            report_data = {
                'metadata': {
                    'report_type': 'data_quality',
                    'generated_at': datetime.now().isoformat(),
                    'version': '1.0',
                    'table_name': analysis_results.get('table_name'),
                    'rows_analyzed': analysis_results.get('rows_analyzed'),
                    'sample_size': analysis_results.get('sample_size')
                },
                'quality_score': {
                    'overall': analysis_results.get('overall_score'),
                    'completeness': analysis_results.get('completeness_score'),
                    'accuracy': analysis_results.get('accuracy_score'),
                    'consistency': analysis_results.get('consistency_score'),
                    'validity': analysis_results.get('validity_score')
                },
                'issues': {
                    'total': len(analysis_results.get('all_issues', [])),
                    'by_severity': self._group_issues_by_severity(analysis_results.get('all_issues', [])),
                    'by_type': self._group_issues_by_type(analysis_results.get('all_issues', []))
                },
                'statistics': analysis_results.get('statistical_analysis', {}),
                'anomalies': analysis_results.get('anomaly_results', {}),
                'recommendations': analysis_results.get('recommendations', {}),
                'profiling_summary': analysis_results.get('profile_summary', {})
            }
            
            return json.dumps(report_data, indent=2, default=str)
            
        except Exception as e:
            logger.error(f"Error generating JSON report: {str(e)}")
            return json.dumps({'error': str(e)})
    
    def generate_pdf_report(self, analysis_results: Dict[str, Any], output_path: str = "report.pdf") -> bool:
        """Generate PDF report"""
        try:
            # Generate HTML report first
            html_content = self.generate_detailed_report(analysis_results)
            
            # Add CSS styling
            styled_html = f"""
            <!DOCTYPE html>
            <html>
            <head>
                <meta charset="UTF-8">
                <title>Data Quality Report</title>
                <style>
                    body {{
                        font-family: Arial, sans-serif;
                        line-height: 1.6;
                        margin: 40px;
                    }}
                    h1, h2, h3 {{
                        color: #2c3e50;
                        border-bottom: 2px solid #3498db;
                        padding-bottom: 10px;
                    }}
                    table {{
                        border-collapse: collapse;
                        width: 100%;
                        margin: 20px 0;
                    }}
                    th, td {{
                        border: 1px solid #ddd;
                        padding: 8px;
                        text-align: left;
                    }}
                    th {{
                        background-color: #f2f2f2;
                    }}
                    .metric-score {{
                        font-size: 3rem;
                        font-weight: bold;
                        text-align: center;
                        margin: 20px 0;
                    }}
                    .critical {{
                        color: #e74c3c;
                    }}
                    .warning {{
                        color: #f39c12;
                    }}
                    .success {{
                        color: #27ae60;
                    }}
                    .footer {{
                        margin-top: 50px;
                        text-align: center;
                        color: #7f8c8d;
                        font-size: 0.9rem;
                    }}
                </style>
            </head>
            <body>
                {html_content}
                <div class="footer">
                    <p>Generated by Data Quality Agent System</p>
                    <p>Report generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                </div>
            </body>
            </html>
            """
            
            # Convert to PDF
            pdfkit.from_string(styled_html, output_path, options={
                'page-size': 'A4',
                'margin-top': '0.75in',
                'margin-right': '0.75in',
                'margin-bottom': '0.75in',
                'margin-left': '0.75in',
                'encoding': "UTF-8",
                'quiet': ''
            })
            
            logger.info(f"PDF report generated: {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"Error generating PDF report: {str(e)}")
            return False
    
    def generate_excel_report(self, analysis_results: Dict[str, Any], output_path: str = "report.xlsx") -> bool:
        """Generate Excel report with multiple sheets"""
        try:
            with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
                # 1. Summary sheet
                summary_data = {
                    'Metric': [
                        'Table Name',
                        'Report Date',
                        'Rows Analyzed',
                        'Overall Quality Score',
                        'Total Issues',
                        'Critical Issues',
                        'Analysis Duration'
                    ],
                    'Value': [
                        analysis_results.get('table_name', 'N/A'),
                        datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        analysis_results.get('rows_analyzed', 0),
                        f"{analysis_results.get('overall_score', 0)}%",
                        len(analysis_results.get('all_issues', [])),
                        len([i for i in analysis_results.get('all_issues', []) if i.get('severity') == 'critical']),
                        analysis_results.get('analysis_duration', 'N/A')
                    ]
                }
                pd.DataFrame(summary_data).to_excel(writer, sheet_name='Summary', index=False)
                
                # 2. Issues sheet
                if analysis_results.get('all_issues'):
                    issues_df = pd.DataFrame(analysis_results['all_issues'])
                    issues_df.to_excel(writer, sheet_name='Issues', index=False)
                
                # 3. Metrics sheet
                metrics_df = pd.DataFrame(self._extract_key_metrics(analysis_results))
                metrics_df.to_excel(writer, sheet_name='Metrics', index=False)
                
                # 4. Recommendations sheet
                recommendations = analysis_results.get('recommendations', {})
                rec_data = []
                for category, items in recommendations.items():
                    for item in items:
                        rec_data.append({
                            'Category': category.replace('_', ' ').title(),
                            'Recommendation': item,
                            'Priority': self._get_priority_for_category(category)
                        })
                if rec_data:
                    pd.DataFrame(rec_data).to_excel(writer, sheet_name='Recommendations', index=False)
                
                # 5. Statistics sheet (if available)
                stats = analysis_results.get('statistical_analysis', {})
                if stats.get('numeric_stats'):
                    numeric_stats_df = pd.DataFrame(stats['numeric_stats']).T
                    numeric_stats_df.to_excel(writer, sheet_name='Numeric Stats')
                
            logger.info(f"Excel report generated: {output_path}")
            return True
            
        except Exception as e:
            logger.error(f"Error generating Excel report: {str(e)}")
            return False
    
    def create_dashboard_data(self, analysis_results: Dict[str, Any]) -> Dict[str, Any]:
        """Create data for interactive dashboard"""
        try:
            dashboard_data = {
                'summary': {
                    'table_name': analysis_results.get('table_name'),
                    'overall_score': analysis_results.get('overall_score'),
                    'row_count': analysis_results.get('row_count'),
                    'issue_count': len(analysis_results.get('all_issues', [])),
                    'critical_issue_count': len([i for i in analysis_results.get('all_issues', []) 
                                               if i.get('severity') == 'critical'])
                },
                'metrics': {
                    'quality_scores': {
                        'overall': analysis_results.get('overall_score'),
                        'completeness': analysis_results.get('completeness_score'),
                        'accuracy': analysis_results.get('accuracy_score'),
                        'consistency': analysis_results.get('consistency_score'),
                        'validity': analysis_results.get('validity_score')
                    },
                    'performance_metrics': analysis_results.get('performance_metrics', {})
                },
                'issues': {
                    'by_severity': self._group_issues_by_severity(analysis_results.get('all_issues', [])),
                    'by_type': self._group_issues_by_type(analysis_results.get('all_issues', [])),
                    'by_column': self._group_issues_by_column(analysis_results.get('all_issues', []))
                },
                'timeline': {
                    'analysis_start': analysis_results.get('analysis_start_time'),
                    'analysis_end': analysis_results.get('analysis_end_time'),
                    'duration': analysis_results.get('analysis_duration')
                },
                'visualizations': {
                    'quality_score_history': analysis_results.get('quality_score_history', []),
                    'issue_trends': analysis_results.get('issue_trends', [])
                }
            }
            
            return dashboard_data
            
        except Exception as e:
            logger.error(f"Error creating dashboard data: {str(e)}")
            return {'error': str(e)}
    
    def _group_issues_by_severity(self, issues: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
        """Group issues by severity"""
        grouped = {}
        for issue in issues:
            severity = issue.get('severity', 'unknown')
            if severity not in grouped:
                grouped[severity] = []
            grouped[severity].append(issue)
        return grouped
    
    def _group_issues_by_type(self, issues: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
        """Group issues by type"""
        grouped = {}
        for issue in issues:
            issue_type = issue.get('issue_type', 'unknown')
            if issue_type not in grouped:
                grouped[issue_type] = []
            grouped[issue_type].append(issue)
        return grouped
    
    def _group_issues_by_column(self, issues: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
        """Group issues by column"""
        grouped = {}
        for issue in issues:
            column = issue.get('column', 'unknown')
            if column not in grouped:
                grouped[column] = []
            grouped[column].append(issue)
        return grouped
    
    def _extract_key_metrics(self, analysis_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract key metrics from analysis results"""
        metrics = []
        
        # Quality scores
        quality_scores = {
            'Overall Quality': analysis_results.get('overall_score'),
            'Completeness': analysis_results.get('completeness_score'),
            'Accuracy': analysis_results.get('accuracy_score'),
            'Consistency': analysis_results.get('consistency_score'),
            'Validity': analysis_results.get('validity_score')
        }
        
        for name, score in quality_scores.items():
            if score is not None:
                metrics.append({
                    'name': name,
                    'value': f"{score:.1f}%",
                    'status': self._get_quality_status(score)
                })
        
        # Issue counts
        all_issues = analysis_results.get('all_issues', [])
        metrics.extend([
            {
                'name': 'Total Issues',
                'value': len(all_issues),
                'status': 'Critical' if len(all_issues) > 10 else 'Warning' if len(all_issues) > 5 else 'Good'
            },
            {
                'name': 'Critical Issues',
                'value': len([i for i in all_issues if i.get('severity') == 'critical']),
                'status': 'Critical' if len([i for i in all_issues if i.get('severity') == 'critical']) > 0 else 'Good'
            }
        ])
        
        return metrics
    
    def _get_quality_color(self, score: float) -> str:
        """Get color based on quality score"""
        if score >= 80:
            return '#27ae60'  # Green
        elif score >= 60:
            return '#f39c12'  # Orange
        else:
            return '#e74c3c'  # Red
    
    def _get_quality_status(self, score: float) -> str:
        """Get status text based on quality score"""
        if score >= 80:
            return 'Excellent'
        elif score >= 70:
            return 'Good'
        elif score >= 60:
            return 'Fair'
        elif score >= 50:
            return 'Poor'
        else:
            return 'Critical'
    
    def _get_priority_for_category(self, category: str) -> str:
        """Get priority level for recommendation category"""
        priority_map = {
            'immediate': 'High',
            'short_term': 'Medium',
            'long_term': 'Low',
            'critical': 'Critical',
            'high': 'High',
            'medium': 'Medium',
            'low': 'Low'
        }
        return priority_map.get(category, 'Medium')
    
    def create_report_package(self, analysis_results: Dict[str, Any], 
                            output_dir: str = "reports") -> Dict[str, str]:
        """Create a complete report package with multiple formats"""
        try:
            output_dir = Path(output_dir)
            output_dir.mkdir(exist_ok=True)
            
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            table_name = analysis_results.get('table_name', 'unknown').replace(' ', '_')
            
            report_files = {}
            
            # 1. HTML Report
            html_report = self.generate_detailed_report(analysis_results)
            html_path = output_dir / f"{table_name}_report_{timestamp}.html"
            with open(html_path, 'w', encoding='utf-8') as f:
                f.write(html_report)
            report_files['html'] = str(html_path)
            
            # 2. Markdown Report
            md_report = self.generate_markdown_report(analysis_results)
            md_path = output_dir / f"{table_name}_report_{timestamp}.md"
            with open(md_path, 'w', encoding='utf-8') as f:
                f.write(md_report)
            report_files['markdown'] = str(md_path)
            
            # 3. JSON Report
            json_report = self.generate_json_report(analysis_results)
            json_path = output_dir / f"{table_name}_report_{timestamp}.json"
            with open(json_path, 'w', encoding='utf-8') as f:
                f.write(json_report)
            report_files['json'] = str(json_path)
            
            # 4. PDF Report
            pdf_path = output_dir / f"{table_name}_report_{timestamp}.pdf"
            if self.generate_pdf_report(analysis_results, str(pdf_path)):
                report_files['pdf'] = str(pdf_path)
            
            # 5. Excel Report
            excel_path = output_dir / f"{table_name}_report_{timestamp}.xlsx"
            if self.generate_excel_report(analysis_results, str(excel_path)):
                report_files['excel'] = str(excel_path)
            
            # 6. Dashboard Data
            dashboard_data = self.create_dashboard_data(analysis_results)
            dashboard_path = output_dir / f"{table_name}_dashboard_{timestamp}.json"
            with open(dashboard_path, 'w', encoding='utf-8') as f:
                json.dump(dashboard_data, f, indent=2, default=str)
            report_files['dashboard'] = str(dashboard_path)
            
            # Create a manifest file
            manifest = {
                'generated_at': datetime.now().isoformat(),
                'table_name': analysis_results.get('table_name'),
                'reports': report_files,
                'summary': {
                    'overall_score': analysis_results.get('overall_score'),
                    'issue_count': len(analysis_results.get('all_issues', [])),
                    'critical_issue_count': len([i for i in analysis_results.get('all_issues', []) 
                                               if i.get('severity') == 'critical'])
                }
            }
            
            manifest_path = output_dir / f"{table_name}_manifest_{timestamp}.json"
            with open(manifest_path, 'w', encoding='utf-8') as f:
                json.dump(manifest, f, indent=2, default=str)
            
            report_files['manifest'] = str(manifest_path)
            
            logger.info(f"Report package created in {output_dir}")
            return report_files
            
        except Exception as e:
            logger.error(f"Error creating report package: {str(e)}")
            return {'error': str(e)}